{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example solution Walkthrough\n",
    "\n",
    "This is an example that shows the steps to be done in order to generate and run the dataset for the problem at hand. We recomend using this notebook as a template for your own implementation.\n",
    "\n",
    "**Before you start** make sure you have read the [README.md](README.md)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Dataset Generation\n",
    "The first step is to generate the dataset. To do so we must define for each sample the graph topology, the routing paths between nodes and its traffic matrix. Then a simulator will use these to calculate the delay and jitter per each path. As a remainder, the training dataset must have no more than 100 unique samples (graph topology, routings and traffic matrix tuple) to be used for training.\n",
    "\n",
    "For more details about the parameters of the dataset, check out the [input_parameters_glossary.ipynb](input_parameters_glossary.ipynb) notebook. For a list of all the contraints placed on these parameters check out the [training_dataset_constraints.md](training_dataset_constraints.md) markdown file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "import random\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Define destination for the generated samples\n",
    "training_dataset_path = \"training\"\n",
    "#paths relative to data folder\n",
    "graphs_path = \"graphs\"\n",
    "routings_path = \"routings\"\n",
    "tm_path = \"tm\"\n",
    "# Path to simulator file\n",
    "simulation_file = os.path.join(training_dataset_path,\"simulation.txt\")\n",
    "# Name of the dataset: Allows you to store several datasets in the same path\n",
    "# Each dataset will be stored at <training_dataset_path>/results/<name>\n",
    "dataset_name = \"dataset1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Create folders\n",
    "if (os.path.isdir(training_dataset_path)):\n",
    "    print (\"Destination path already exists. Files within the directory may be overwritten.\")\n",
    "else:\n",
    "    os.makedirs(os.path.join(training_dataset_path,graphs_path))\n",
    "    os.mkdir(os.path.join(training_dataset_path,routings_path))\n",
    "    os.mkdir(os.path.join(training_dataset_path,tm_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Generate a graph topology file. The graphs generated have the following characteristics:\n",
    "- All nodes have buffer sizes of 32000 bits and FIFO scheduling\n",
    "- All links have bandwidths of 100000 bits per second\n",
    "'''\n",
    "def generate_topology(net_size, graph_file):\n",
    "    G = nx.Graph()\n",
    "    nodes = []\n",
    "    node_degree = []\n",
    "    for i in range(net_size):\n",
    "        node_degree.append(random.choices([2,3,4,5,6],weights=[0.34,0.35,0.2,0.1,0.01])[0])\n",
    "        \n",
    "        nodes.append(i)\n",
    "        G.add_node(i)\n",
    "        # Assign to each node the scheduling Policy\n",
    "        G.nodes[i][\"schedulingPolicy\"] = \"FIFO\"\n",
    "        # Assign the buffer size of all the ports of the node\n",
    "        G.nodes[i][\"bufferSizes\"] = 32000\n",
    "\n",
    "    finish = False\n",
    "    while (True):\n",
    "        aux_nodes = list(nodes)\n",
    "        n0 = random.choice(aux_nodes)\n",
    "        aux_nodes.remove(n0)\n",
    "        # Remove adjacents nodes (only one link between two nodes)\n",
    "        for n1 in G[n0]:\n",
    "            if (n1 in aux_nodes):\n",
    "                aux_nodes.remove(n1)\n",
    "        if (len(aux_nodes) == 0):\n",
    "            # No more links can be added to this node - can not acomplish node_degree for this node\n",
    "            nodes.remove(n0)\n",
    "            if (len(nodes) == 1):\n",
    "                break\n",
    "            continue\n",
    "        n1 = random.choice(aux_nodes)\n",
    "        G.add_edge(n0, n1)\n",
    "        # Assign the link capacity to the link\n",
    "        G[n0][n1][\"bandwidth\"] = 100000\n",
    "        \n",
    "        for n in [n0,n1]:\n",
    "            node_degree[n] -= 1\n",
    "            if (node_degree[n] == 0):\n",
    "                nodes.remove(n)\n",
    "                if (len(nodes) == 1):\n",
    "                    finish = True\n",
    "                    break\n",
    "        if (finish):\n",
    "            break\n",
    "    if (not nx.is_connected(G)):\n",
    "        G = generate_topology(net_size, graph_file)\n",
    "        return G\n",
    "    \n",
    "    nx.write_gml(G,graph_file)\n",
    "    \n",
    "    return (G)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Generate a file with the shortest path routing of the topology G\n",
    "'''\n",
    "def generate_routing(G, routing_file):\n",
    "    with open(routing_file,\"w\") as r_fd:\n",
    "        lPaths = nx.shortest_path(G)\n",
    "        for src in G:\n",
    "            for dst in G:\n",
    "                if (src == dst):\n",
    "                    continue\n",
    "                path =  ','.join(str(x) for x in lPaths[src][dst])\n",
    "                r_fd.write(path+\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Generate a traffic matrix file. We consider flows between all nodes in the newtork, each with the following characterstics\n",
    "- The average bandwidth ranges between 10 and max_avg_lbda\n",
    "- We consider three time distributions (in case of the ON-OFF policy we have off periods of 10 and on periods of 5)\n",
    "- We consider two packages distributions, chosen at random\n",
    "- ToS is assigned randomly\n",
    "'''\n",
    "def generate_tm(G,max_avg_lbda, traffic_file):\n",
    "    poisson = \"0\" \n",
    "    cbr = \"1\"\n",
    "    on_off = \"2,10,5\" #time_distribution, avg off_time exp, avg on_time exp\n",
    "    time_dist = [poisson,cbr,on_off]\n",
    "    \n",
    "    pkt_dist_1 = \"0,300,0.5,1700,0.5\" #genric pkt size dist, pkt_size 1, prob 1, pkt_size 2, prob 2\n",
    "    pkt_dist_2 = \"0,500,0.6,1000,0.2,1400,0.2\" #genric pkt size dist, pkt_size 1, prob 1, \n",
    "                                               # pkt_size 2, prob 2, pkt_size 3, prob 3\n",
    "    pkt_size_dist = [pkt_dist_1, pkt_dist_2]\n",
    "    tos_lst = [0,1,2]\n",
    "    \n",
    "    with open(traffic_file,\"w\") as tm_fd:\n",
    "        for src in G:\n",
    "            for dst in G:\n",
    "                avg_bw = random.randint(10,max_avg_lbda)\n",
    "                td = random.choice(time_dist)\n",
    "                sd = random.choice(pkt_size_dist)\n",
    "                tos = random.choice(tos_lst)\n",
    "                \n",
    "                traffic_line = \"{},{},{},{},{},{}\".format(\n",
    "                    src,dst,avg_bw,td,sd,tos)\n",
    "                tm_fd.write(traffic_line+\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "We generate the files using the previously defined functions. This code will produce 100 samples where:\n",
    "- We generate 5 topologies, and then we generate 20 traffic matrices for each\n",
    "- The topology sizes range from 6 to 10 nodes\n",
    "- We consider the maximum average bandwidth per flow as 1000\n",
    "\"\"\"\n",
    "max_avg_lbda = 1000\n",
    "with open (simulation_file,\"w\") as fd:\n",
    "    for net_size in range (6,11):\n",
    "        #Generate graph\n",
    "        graph_file = os.path.join(graphs_path,\"graph_{}.txt\".format(net_size))\n",
    "        G = generate_topology(net_size, os.path.join(training_dataset_path,graph_file))\n",
    "        # Generate routing\n",
    "        routing_file = os.path.join(routings_path,\"routing_{}.txt\".format(net_size))\n",
    "        generate_routing(G, os.path.join(training_dataset_path,routing_file))\n",
    "        # Generate TM:\n",
    "        for i in range (20):\n",
    "            tm_file = os.path.join(tm_path,\"tm_{}_{}.txt\".format(net_size,i))\n",
    "            generate_tm(G,max_avg_lbda, os.path.join(training_dataset_path,tm_file))\n",
    "            sim_line = \"{},{},{}\\n\".format(graph_file,routing_file,tm_file)   \n",
    "            # If dataset has been generated in windows, convert paths into linux format\n",
    "            fd.write(sim_line.replace(\"\\\\\",\"/\"))  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "At this point, we have created the input files of the simulator and we prepare the docker to obtain the results.\n",
    "\n",
    "**Note**: the docker image is saved in Dockerhub. When running the \"docker run\" command for the first time, the image will be downloaded automatically. This does not require more actions by the user, other that making sure the computer can connect to the internet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# First we generate the configuration file\n",
    "import yaml\n",
    "\n",
    "conf_file = os.path.join(training_dataset_path,\"conf.yml\")\n",
    "conf_parameters = {\n",
    "    \"threads\": 6,# Number of threads to use \n",
    "    \"dataset_name\": dataset_name, # Name of the dataset. It is created in <training_dataset_path>/results/<name>\n",
    "    \"samples_per_file\": 10, # Number of samples per compressed file\n",
    "    \"rm_prev_results\": \"n\", # If 'y' is selected and the results folder already exists, the folder is removed.\n",
    "}\n",
    "\n",
    "with open(conf_file, 'w') as fd:\n",
    "    yaml.dump(conf_parameters, fd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from getpass import getpass\n",
    "def docker_cmd(training_dataset_path):\n",
    "    raw_cmd = f\"docker run --rm --mount type=bind,src={os.path.join(os.getcwd(),training_dataset_path)},dst=/data bnnupc/netsim:v0.1\"\n",
    "    terminal_cmd = raw_cmd\n",
    "    if os.name != 'nt': # Unix, requires sudo\n",
    "        print(\"Superuser privileges are required to run docker. Introduce sudo password when prompted\")\n",
    "        terminal_cmd = f\"echo {getpass()} | sudo -S \" + raw_cmd\n",
    "        raw_cmd = \"sudo \" + raw_cmd\n",
    "    return raw_cmd, terminal_cmd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Start the docker\n",
    "raw_cmd, terminal_cmd = docker_cmd(training_dataset_path)\n",
    "print(\"The next cell will launch docker from the notebook. Alternatively, run the following command from a terminal:\")\n",
    "print(raw_cmd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Sometimes it may happen that the execution cell will not produce an output until it ends. If this is the case, and you have docker desktop, you can look at the outputs from there (by going into containers, selecting 'bnnupc/netsim', and then into logs).\n",
    "\n",
    "It is recommended to check the log file to validate the status of the simulation. This file should contain one line per simulated sample where the first value is the simulation line and then Ok if the simulation finishes properly, or an error message otherwise. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "!{terminal_cmd}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Visualizing the dataset\n",
    "After generating the dataset, it is recommended to then analyze it to guarantee its quality (e.g. it is balanced, covers edge cases...). To do so we refer to the notebook [dataset_visualization.ipynb](dataset_visualization.ipynb) where some code is provided in how this can be done. Feel free to expand the functions already there!\n",
    "\n",
    "### Training the model\n",
    "\n",
    "At this point, the samples are generated in the location specified at the beginning of the notebook, and we can now train the model.\n",
    "To train the model, just import the module RouteNet_Fermi and run its function ```main(train_path, final_evaluation = False)```. For more details about the RouteNet_Fermi's API, check out the [README.md](README.md) file.\n",
    "\n",
    "**NOTE**: The training is fixed to 20 epochs and it produces 20 different model checkpoints. Participants can select the best model generated during training (not necessarily the last one) and use it for their submissions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Train the model\n",
    "from RouteNet_Fermi import main\n",
    "main(\"./training\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Once trained we can also use the function ```evaluate(ckpt_path)``` to load a single checkpoint and evaluate it individually. Once again for more details check out the [README.md](README.md) file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from RouteNet_Fermi import evaluate\n",
    "\n",
    "t = evaluate(\"modelCheckpoints/02-35.07\")"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "e4fabe4be1dcb5b95007215d13ed47b80f9ccf78939eea74ae4a681230c3cbef"
  },
  "kernelspec": {
   "display_name": "Python 3.9.0 ('BNNChallenge2022')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
